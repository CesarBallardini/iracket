{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Bayesian concept learning\n",
    "\n",
    "Suppose we have a population of objects, each represented by a vector of features.  Similar objects have similar feature vectors.  We get only a few positive examples of a concept--say \"dog\"--that we want generalize correctly.  Which of the other objects are dogs?\n",
    "\n",
    "The model below is a fuzzy variant of \"Bayesian concept learning\", as proposed by Josh Tenenbaum in his dissertation (ref).  Rather than assuming the objects are either in or not in the extension of the to-be-learned concept, we assume that each has some degree of membership.  The probability that an object is drawn as an example is proportional to its degree of \"in-ness\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Some data to play with\n",
    "\n",
    "We populate a hash table whose keys are object name strings and whose values are eleven-dimensional vectors of real numbers.  Vectors for 96 objects have been constructed from about 2000 rdf (subject-verb-object) triples describing them, by means that would take us too far afield to discuss.  Suffice it to say that objects that occur in similar contexts have similar vectors.  \n",
    "\n",
    "TODO: fix the csv utils so I don't have to do all the trimming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(require gamble\n",
    "         gamble/util/csv\n",
    "         racket/string\n",
    "         racket/vector)\n",
    "\n",
    "(define object-codes \n",
    "  (make-hash\n",
    "      (map \n",
    "       (lambda (row)\n",
    "         (cons (string-trim (vector-ref row 0))\n",
    "               (map (lambda (n) \n",
    "                      (string->number \n",
    "                       (string-trim n)))\n",
    "                   (vector->list (vector-drop row 1)))))\n",
    "       (read-csv-file \"toy_vectors.csv\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Having a look at the object vectors\n",
    "\n",
    "If we look at the cosine similarity of object vectors, we can see that our intuitions of relative similarity are respected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ";; Cosine.\n",
    "(define (similarity obj1 obj2)\n",
    "  (let ([v1 (hash-ref object-codes obj1)]\n",
    "        [v2 (hash-ref object-codes obj2)])\n",
    "    (let ([l1 (sqrt (apply + (map * v1 v1)))]\n",
    "          [l2 (sqrt (apply + (map * v2 v2)))])\n",
    "     (/ (apply + (map * v1 v2)) (* l1 l2)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog1 (individual) / dog2 (individual): 0.9962239277160961\n",
      "dog1 (individual) / dog (class): 0.8802545473616828\n",
      "dog (class) / theft (class): 0.9999341458378703\n",
      "dog1 (individual) / subPropertyOf (higher-order relation): 0.7880343956261665\n",
      "theft (class) /  transfer (super-class): 0.9996345287318831"
     ]
    }
   ],
   "source": [
    "(printf \"dog1 (individual) / dog2 (individual): ~a\\ndog1 (individual) / dog (class): ~a\\ndog (class) / theft (class): ~a\\ndog1 (individual) / subPropertyOf (higher-order relation): ~a\\ntheft (class) /  transfer (super-class): ~a\"\n",
    "        (similarity \"toy_dog1\" \"toy_dog2\")\n",
    "        (similarity \"toy_dog1\" \"toy_Dog\")\n",
    "        (similarity \"toy_Dog\" \"toy_Theft\")\n",
    "        (similarity \"toy_dog1\" \"rdfs_subPropertyOf\")\n",
    "        (similarity \"toy_Theft\" \"toy_Transfer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Utilities\n",
    "\n",
    "The logistic function is probably familiar.  The second function is a bit of awkwardness intended to simplify the generative model specification inside the sampler, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(define (logistic x) (/ 1.0 (+ 1.0 (exp (- x)))))\n",
    "\n",
    ";; Awkward...\n",
    "(define (squashed-dot-prod x-list y-fn)\n",
    "  (logistic (for/sum ([i (length x-list)]) \n",
    "        (* (list-ref x-list i) (y-fn i)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The model\n",
    "\n",
    "We see several examples, which are assumed to be drawn thusly: \n",
    "\n",
    "1. There is some weight vector that defines \"the concept\".  Each element of the weight vector is randomly either zero or a normal draw.\n",
    "\n",
    "2. The dot product of the weight vector and an object's feature vector is passed through a logistic \"squashing\" function, producing a number in $(0,1)$ which we take to be the degree to which the object is \"in\" the concept.\n",
    "\n",
    "3. The vector of \"in-nesses\" is normalized (automatically, in construction) to produce a discrete distribution over objects, from which examples are then drawn.\n",
    "\n",
    "Inference yields in-nesses for several objects, given weights sampled from the posterior, which will favor concentrating in-ness on the examples, and (within the strong smoothness constraint imposed by the model form) avoiding it on non-examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(define bcl-sampler\n",
    "  (mh-sampler\n",
    "   \n",
    "   ;;;;;;;  Generative model ;;;;;;;;;\n",
    "   \n",
    "   ;; The weight vector that defines the concept.\n",
    "   (defmem (wt i) (if (flip 0.5) 0 (normal 0 3.0)))\n",
    "   \n",
    "   ;; The set of squashed dot products that defines the concept's discrete distribution.\n",
    "   (deflazy weighted-objects\n",
    "      (map \n",
    "         (lambda (obj)\n",
    "            (cons obj (squashed-dot-prod (hash-ref object-codes obj) wt)))\n",
    "         (hash-keys object-codes)))\n",
    "\n",
    "   ;; Drawing examples from that distribution.\n",
    "   (defmem (examples k) (discrete weighted-objects))\n",
    "   \n",
    "   \n",
    "   ;;;;;;;;; Observations ;;;;;;;;;;;\n",
    "   \n",
    "   (observe (examples 1) \"toy_dog1\")\n",
    "   (observe (examples 2) \"toy_dog2\")\n",
    "   (observe (examples 3) \"toy_dog11\")\n",
    "   (observe (examples 4) \"toy_dog12\")\n",
    "   (observe (examples 5) \"toy_dog4\")\n",
    "   (observe (examples 6) \"toy_dog14\")\n",
    "   \n",
    "   ;(observe (examples 1) \"toy_giveEvt1\")\n",
    "   ;(observe (examples 2) \"toy_theft1\")\n",
    "   \n",
    "   \n",
    "   ;;;;;;;;; Query ;;;;;;;;;;;;;;;\n",
    "   \n",
    "   (vector \n",
    "    (squashed-dot-prod (hash-ref object-codes \"toy_dog4\") wt)\n",
    "    (squashed-dot-prod (hash-ref object-codes \"toy_dog3\") wt)\n",
    "    (squashed-dot-prod (hash-ref object-codes \"toy_dog13\") wt)\n",
    "    (squashed-dot-prod (hash-ref object-codes \"toy_person3\") wt)\n",
    "    (squashed-dot-prod (hash-ref object-codes \"toy_person13\") wt)\n",
    "    (squashed-dot-prod (hash-ref object-codes \"toy_theft1\") wt)\n",
    "    (squashed-dot-prod (hash-ref object-codes \"toy_giveEvt2\") wt)\n",
    "    (squashed-dot-prod (hash-ref object-codes \"rdfs_subPropertyOf\") wt))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(define smpls (sampler->mean bcl-sampler 50 #:burn 2000 #:thin 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Results\n",
    "\n",
    "Yea!! As we hoped for, generalization is strongest to dogs (the left-most three bars, only the first of which is an example), and next-strongest to people (the next two bars to the right).  Generalization is very low to a theft event and a gift event (the next two), and non-existent to the abstract property \"subPropertyOf\".\n",
    "\n",
    "This is significant, if simple, learning from just a few examples, with no negative examples at all, based on a model no more complex than logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(require \"c3_helpers.rkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/x-c3-data": [
       "{\"data\":{\"type\":\"bar\",\"xs\":{\"ys1\":\"xs1\"},\"columns\":[[\"xs1\",\"dog4\",\"dog3\",\"dog13\",\"person3\",\"person13\",\"theft1\",\"gift2\",\"subPropertyOf\"],[\"ys1\",0.6102800518094778,0.6103132619332787,0.6103097093279195,0.26620001981665964,0.3339283985447949,0.034404085447293384,0.033901983378010134,0.009646183356972746]]},\"axis\":{\"x\":{\"type\":\"category\",\"label\":\"object\",\"tick\":{\"rotate\":90}},\"y\":{\"label\":\"mean in-ness\"}}}"
      ],
      "text/plain": [
       "(c3-data . #hasheq((data . #hasheq((type . bar) (xs . #hasheq((ys1 . xs1))) (columns . ((xs1 dog4 dog3 dog13 person3 person13 theft1 gift2 subPropertyOf) (ys1 0.6102800518094778 0.6103132619332787 0.6103097093279195 0.26620001981665964 0.3339283985447949 0.034404085447293384 0.033901983378010134 0.009646183356972746))))) (axis . #hasheq((x . #hasheq((type . category) (label . object) (tick . #hasheq((rotate . 90))))) (y . #hasheq((label . mean in-ness)))))))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bar-c3-categorical \n",
    " (list 1 2 3 4 5 6 7 8)\n",
    " (vector->list smpls)\n",
    " (list \"dog4\" \"dog3\" \"dog13\" \"person3\" \"person13\" \"theft1\" \"gift2\" \"subPropertyOf\")\n",
    " #:xlabel \"object\"\n",
    " #:ylabel \"mean in-ness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Racket",
   "language": "racket",
   "name": "racket"
  },
  "language_info": {
   "codemirror_mode": "scheme",
   "file_extension": ".rkt",
   "mimetype": "text/x-racket",
   "name": "Racket",
   "pygments_lexer": "racket",
   "version": "6.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
